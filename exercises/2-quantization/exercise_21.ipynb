{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detected-pipeline",
   "metadata": {},
   "source": [
    "## Embedded ML Lab - Excercise 2 - Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-texture",
   "metadata": {},
   "source": [
    "The goal of this exercise is to take a given network, fuse its operators, and finally quantize it. For that we will do the following steps\n",
    "* 1) We define the quantized network with fused operators\n",
    "* 2) We determine how to fuse `conv-bn-relu` structures into a single quantized operation.\n",
    "* 3) We fuse the weights from the pre-trained state dict and quantize them\n",
    "* 4) We use a calibration batch from the pretrained network to determine all required scales\n",
    "* 5) Done :)\n",
    "\n",
    "For this lab the non-quantized version of the net we use is already implemented in `net.py`. It contains 6 conv, 6 batchnorm, 6 relu layers, and only has a very small linear part at the end. Take a look at it.\n",
    "\n",
    "<img src=\"src/cifarnet.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "integrated-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import CifarNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dress-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "tf = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "testloader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('data/', train=False, download=True, transform=tf), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mailman",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "To measure the effects of quantization we want to measure the time it takes to calculate a batch with the quantized and the unquatized network to run on the cpu. Additionally, we want to know what the accuracy penalty is.\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Implement a function `net_time` that measures the time it takes (forward pass) to process a batch with size 32 of cifar100. You can use `t_now = time.time()` to get the current time.</span>\n",
    "    * <span style=\"color:green\">NOTE: To save time, you do not have to iterate over the whole dataset.</span>\n",
    "* <span style=\"color:green\">Implement a function `net_acc` that measures the accuracy of the net class, and takes the class type, a state_dict, and a dataloader as input.</span>\n",
    "    * <span style=\"color:green\">NOTE: To save time, you do not have to iterate over the whole dataset.</span>\n",
    "    * <span style=\"color:green\">NOTE: You can reuse code from the last lab exercises.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "included-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def net_time(model_class, testloader):\n",
    "    \n",
    "    #----to-be-done-by-student-------------------\n",
    "    ###\n",
    "    ###\n",
    "    #----to-be-done-by-student-------------------\n",
    "    t = 0.0\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model_class()\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        inputs, _ = next(iter(testloader))  # Get a batch of inputs\n",
    "        inputs = inputs.to(device)\n",
    "        t_start = time.time()  # Record the start time\n",
    "        _ = model(inputs)  # Perform forward pass\n",
    "        t_end = time.time()  # Record the end time\n",
    "        t = t + t_end - t_start  # Calculate elapsed time\n",
    "    return t\n",
    "\n",
    "def net_acc(model_class, state_dict, testloader):\n",
    "    #----to-be-done-by-student-------------------\n",
    "    ###\n",
    "    ###\n",
    "    #----to-be-done-by-student-------------------\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #accuracy = 0.0\n",
    "    #accuracy = test(model, test_loader, device=device)\n",
    "    model = model_class()\n",
    "    model.load_state_dict(state_dict)\n",
    "    #model.eval()  # Set the model to evaluation mode\n",
    "    for idx, (inputs, targets) in enumerate(testloader):\n",
    "        outputs = model(inputs)\n",
    "        _, out_class = torch.max(outputs, dim=1)\n",
    "        correct += (out_class==targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        if (idx==6):\n",
    "            break\n",
    "    accuracy = correct / total\n",
    "        \n",
    "    #with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    #    for inputs, labels in testloader:\n",
    "    #        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #        outputs = model(inputs)  # Perform forward pass\n",
    "    #        _, predicted = torch.max(outputs, 1)\n",
    "    #        total += labels.size(0)\n",
    "    #        correct += (predicted == labels).sum().item()\n",
    "    #accuracy = correct / total\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "periodic-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time unquantized: 1.3777589797973633 s\n",
      "Accuracy unquantized: 78.0600%\n"
     ]
    }
   ],
   "source": [
    "print(f'Time unquantized: {net_time(CifarNet, testloader)} s')\n",
    "print(f\"Accuracy unquantized: {net_acc(CifarNet, torch.load('state_dict.pt'), testloader):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-priority",
   "metadata": {},
   "source": [
    "## Quantized network\n",
    "Now we define the quantized version of CifarNet with fused operators ( conv-bn-relu -> qfused_conv_relu). The resulting network has a structure as shown below:\n",
    "\n",
    "<img src=\"src/cifarnet_quantized.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Take the provided image as well as the CifarNet implementation as reference and implemenet the **forward pass** of QCifarNet.</span>\n",
    "    * <span style=\"color:green\">The required modules `Conv2drelu` and `QLinear` are already provided and can be used like any other module we have seen before. Note that these modules require their weights to be quantized, the bias is unquantized. The forward pass of these modules require an quantized input and return an quantized output. The modules are essentially only a wrapper with parameters around `torch.ops.quantized.conv2d_relu` and `torch.ops.quantized.linear`. Additionally these modules have an paramter called `scale`, that is used as ouput scale for the operation.</span>\n",
    "    * <span style=\"color:green\">You might require some other \"stateless\" operators such as `torch.quantize_per_tensor`, `torch.dequantize`,`torch.flatten`, and `torch.nn.quantized.functional.max_pool2d`.</span>\n",
    "* <span style=\"color:green\">Profile the resulting net and compare its forward pass time to the non-quantized implementation.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sudden-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def f_sd(sd, endswith_key_string):\n",
    "    keys = [i for i in sd.keys() if i.endswith(endswith_key_string)]\n",
    "    if not keys:\n",
    "        raise KeyError(endswith_key_string)\n",
    "    return sd[keys[0]]\n",
    "\n",
    "#Quantized Conv2dReLU Module\n",
    "class QConv2dReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(QConv2dReLU, self).__init__()\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.quantize_per_tensor(torch.Tensor(\n",
    "                out_channels, in_channels // 1, *(kernel_size, kernel_size)), scale=0.1, zero_point = 0, dtype=torch.qint8), requires_grad=False)\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_channels), requires_grad=False)\n",
    "\n",
    "        self.register_buffer('scale', torch.tensor(0.1))\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self._prepack = self._prepare_prepack(self.weight, self.bias, stride, padding)\n",
    "        self._register_load_state_dict_pre_hook(self._sd_hook)\n",
    "\n",
    "    def _prepare_prepack(self, qweight, bias, stride, padding):\n",
    "        assert qweight.is_quantized, \"QConv2dReLU requires a quantized weight.\"\n",
    "        assert not bias.is_quantized, \"QConv2dReLU requires a float bias.\"\n",
    "        return torch.ops.quantized.conv2d_prepack(qweight, bias, stride=[stride, stride], dilation=[1,1], padding=[padding, padding], groups=1)\n",
    "\n",
    "    \n",
    "    def _sd_hook(self, state_dict, prefix, *_):\n",
    "        self._prepack = self._prepare_prepack(f_sd(state_dict, prefix + 'weight'), f_sd(state_dict, prefix + 'bias'),\n",
    "                                             self.stride, self.padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.ops.quantized.conv2d_relu(x, self._prepack, self.scale, 64)\n",
    "\n",
    "    \n",
    "#Quantized Linear Module\n",
    "class QLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(QLinear, self).__init__()\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.quantize_per_tensor(torch.Tensor(out_features, in_features), scale=0.1, zero_point = 0, dtype=torch.qint8), requires_grad=False)\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        self.register_buffer('scale', torch.tensor(0.1))\n",
    "        \n",
    "        self._prepack = self._prepare_prepack(self.weight, self.bias)\n",
    "        \n",
    "        self._register_load_state_dict_pre_hook(self._sd_hook)\n",
    "        \n",
    "    def _prepare_prepack(self, qweight, bias):\n",
    "        assert qweight.is_quantized, \"QConv2dReLU requires a quantized weight.\"\n",
    "        assert not bias.is_quantized, \"QConv2dReLU requires a float bias.\"\n",
    "        return torch.ops.quantized.linear_prepack(qweight, bias)\n",
    "    \n",
    "    def _sd_hook(self, state_dict, prefix, *_):\n",
    "        self._prepack = self._prepare_prepack(f_sd(state_dict, prefix + 'weight'), f_sd(state_dict, prefix + 'bias'))\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.ops.quantized.linear(x, self._prepack, self.scale, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict of QConv2dReLU\n",
      "weight torch.qint8\n",
      "bias torch.float32\n",
      "scale torch.float32\n",
      "\n",
      "state_dict of QLinear\n",
      "weight torch.qint8\n",
      "bias torch.float32\n",
      "scale torch.float32\n"
     ]
    }
   ],
   "source": [
    "print('state_dict of QConv2dReLU')\n",
    "qconv2drelu = QConv2dReLU(3, 16)\n",
    "for key in qconv2drelu.state_dict(): print(key, qconv2drelu.state_dict()[key].dtype)\n",
    "print('\\nstate_dict of QLinear')\n",
    "qlinear = QLinear(10, 10)\n",
    "for key in qlinear.state_dict(): print(key, qlinear.state_dict()[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorrect-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCifarNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QCifarNet, self).__init__()\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.tensor(0.1))\n",
    "\n",
    "        self.conv1 = QConv2dReLU(3, 16, 3, 1, padding=1)\n",
    "        self.conv2 = QConv2dReLU(16,16, 3, 1, padding=1)\n",
    "\n",
    "        self.conv3 = QConv2dReLU(16, 32, 3, 1, padding=1)\n",
    "        self.conv4 = QConv2dReLU(32, 32, 3, 1, padding=1)\n",
    "\n",
    "        self.conv5 = QConv2dReLU(32, 64, 3, 1, padding=1)\n",
    "        self.conv6 = QConv2dReLU(64, 64, 3, 1, padding=1)\n",
    "\n",
    "        self.fc = QLinear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #to-be-done-by-student\n",
    "        ###\n",
    "        ###\n",
    "        #to-be-done-by-student\n",
    "        x = torch.quantize_per_tensor(x, scale=self.scale, zero_point=64, dtype=torch.quint8)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.quantized.functional.max_pool2d(x, 2, stride=2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.quantized.functional.max_pool2d(x, 2, stride=2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = torch.nn.quantized.functional.max_pool2d(x, 2, stride=2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.dequantize(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "unexpected-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time quantized: 0.1098482608795166 s\n"
     ]
    }
   ],
   "source": [
    "#We evaulate how fast the quantized verions of CifarNet is\n",
    "print(f\"Time quantized: {net_time(QCifarNet, testloader)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-occasion",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calibration and Operator Fusion\n",
    "\n",
    "First we focus on the operator fusion:\n",
    "* We need calculate the new weights (fused conv and batchnorm weights). After we have weights, we can quantize them using the `tensor_scale` equation from earlier.\n",
    "    * A Conv2d convolution can be expressed as $y_i = \\boldsymbol{ W_{i}} \\star x + b_{_i}$, where $y_i$ is the channel wise output of the convolution and $\\boldsymbol{ W_{i}}$ is a $\\text{channel_in} \\times \\text{kernel_size} \\times \\text{kernel_size}$ kernel.\n",
    "    * The batch_norm operation looks like this: $\\hat x_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$, where for each output channel of a convolution $i \\in C$, we scale and shift the input to be zero mean and unit variance, where $\\mu_i$ is the channel wise input mean, and $\\sigma^2_i$ is the channels wise variance. Parameter $\\epsilon$ is added for numerical stability.\n",
    "    * After this shift and scale operation trainable weight and bias terms are added\n",
    " $y_i = \\gamma_i \\hat x_i + \\beta_i$, where $\\gamma_i$ is a channel wise scale factor and $\\beta_i$ is a channel wise bias.\n",
    "    * We can express the batchnorm operation as $y_i = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}})x_i +  (\\frac{ - \\mu_i \\gamma_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$ and fuse it with the convolution kernel by using $y_i = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}} \\boldsymbol{ W_i}) \\star x_i +  (\\frac{ \\gamma_i ( b_i - \\mu_i)}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$, s.t. the fused kernel (output channel wise) can be expressed as $\\tilde{\\boldsymbol{ W_{i}}} = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}}) \\boldsymbol{ W_i}$ and the fused bias (output channel wise) as $\\tilde{b_i} = (\\frac{ \\gamma_i ( b_i - \\mu_i)}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$ .\n",
    " \n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Implement a function `fuse_conv_bn_weights` that fuses the weights and bias of the convolution with the weights, bias, running_mean and running_var of the batchnorm_layer</span>\n",
    "    * <span style=\"color:green\"> determine $\\tilde{b}$ and $\\tilde{\\boldsymbol{ W}}$</span>\n",
    "    * <span style=\"color:green\"> You can either do this channel by channel or compleatly vectorized</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "linear-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_scale(input):\n",
    "    return float(2*torch.max(torch.abs(torch.max(input)), torch.abs(torch.min(input))))/127.0\n",
    "\n",
    "def fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_w, bn_b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        conv_w: shape=(output_channels, in_channels, kernel_size, kernel_size)\n",
    "        conv_b: shape=(output_channels)\n",
    "        bn_rm:  shape=(output_channels)\n",
    "        bn_rv:  shape=(output_channels)\n",
    "        bn_w:   shape=(output_channels)\n",
    "        bn_b:   shape=(output_channels)\n",
    "    \n",
    "    Output:\n",
    "        fused_conv_w = shape=conv_w\n",
    "        fused_conv_b = shape=conv_b\n",
    "    \"\"\"\n",
    "    bn_eps = 1e-05\n",
    "\n",
    "    fused_conv = torch.zeros(conv_w.shape)\n",
    "    fused_bias = torch.zeros(conv_b.shape)\n",
    "    \n",
    "    #to-be-done-by-student\n",
    "    ###\n",
    "    ###\n",
    "    #to-be-done-by-student\n",
    "    for i in range(conv_w.shape[0]):\n",
    "        gamma = bn_w[i] / torch.sqrt(bn_rv[i] + bn_eps)\n",
    "        fused_conv[i] = conv_w[i] * gamma.view(-1, 1, 1, 1)\n",
    "        fused_bias[i] = gamma * (conv_b[i] - bn_rm[i]) / torch.sqrt(bn_rv[i] + bn_eps) + bn_b[i]\n",
    "        \n",
    "    # Quantize the fused weights and bias\n",
    "    #scale_w = tensor_scale(fused_conv)\n",
    "    #scale_b = tensor_scale(fused_bias)\n",
    "\n",
    "    #fused_conv = torch.quantize_per_tensor(fused_conv, scale=scale_w, zero_point=0, dtype=torch.qint8)\n",
    "    #fused_bias = torch.quantize_per_tensor(fused_bias, scale=scale_b, zero_point=0, dtype=torch.qint8)\n",
    "    \n",
    "\n",
    "    return fused_conv, fused_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-clark",
   "metadata": {},
   "source": [
    "Now that we know how to fuse conv and batchnorm layers, we can setup the quantized state dict. We have to take the unfused unquantized parameters of the unquantized pretrained network (`state_dict.pt`) and fuse and quantize them.\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>  \n",
    "* <span style=\"color:green\">Now for each Conv weights and biases, load the pre-trained float weights and biases from the saved state_dict, fuse the corresponding weights and biases with the batch norm weights, biases, mean, and variance, and store the fused quantized weight into the quantized state_dict `qsd`</span>\n",
    "* <span style=\"color:green\">Some Tips:</span>\n",
    "    * <span style=\"color:green\">Print out the keys from the unquantized and quantized state_dict and see what is inside.</span>\n",
    "    * <span style=\"color:green\">You can ignore the scales for now, we will take care of them later.</span>\n",
    "    * <span style=\"color:green\">Reuse the function `tensor_scale`</span>\n",
    "    * <span style=\"color:green\">Weights require to be of type torch.qint8, therefor have a zero_point of 0.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "established-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale torch.float32\n",
      "conv1.weight torch.qint8\n",
      "conv1.bias torch.float32\n",
      "conv1.scale torch.float32\n",
      "conv2.weight torch.qint8\n",
      "conv2.bias torch.float32\n",
      "conv2.scale torch.float32\n",
      "conv3.weight torch.qint8\n",
      "conv3.bias torch.float32\n",
      "conv3.scale torch.float32\n",
      "conv4.weight torch.qint8\n",
      "conv4.bias torch.float32\n",
      "conv4.scale torch.float32\n",
      "conv5.weight torch.qint8\n",
      "conv5.bias torch.float32\n",
      "conv5.scale torch.float32\n",
      "conv6.weight torch.qint8\n",
      "conv6.bias torch.float32\n",
      "conv6.scale torch.float32\n",
      "fc.weight torch.qint8\n",
      "fc.bias torch.float32\n",
      "fc.scale torch.float32\n",
      "-------end------\n",
      "conv1.weight torch.float32\n",
      "conv1.bias torch.float32\n",
      "conv2.weight torch.float32\n",
      "conv2.bias torch.float32\n",
      "conv3.weight torch.float32\n",
      "conv3.bias torch.float32\n",
      "conv4.weight torch.float32\n",
      "conv4.bias torch.float32\n",
      "conv5.weight torch.float32\n",
      "conv5.bias torch.float32\n",
      "conv6.weight torch.float32\n",
      "conv6.bias torch.float32\n",
      "bn1.weight torch.float32\n",
      "bn1.bias torch.float32\n",
      "bn1.running_mean torch.float32\n",
      "bn1.running_var torch.float32\n",
      "bn1.num_batches_tracked torch.int64\n",
      "bn2.weight torch.float32\n",
      "bn2.bias torch.float32\n",
      "bn2.running_mean torch.float32\n",
      "bn2.running_var torch.float32\n",
      "bn2.num_batches_tracked torch.int64\n",
      "bn3.weight torch.float32\n",
      "bn3.bias torch.float32\n",
      "bn3.running_mean torch.float32\n",
      "bn3.running_var torch.float32\n",
      "bn3.num_batches_tracked torch.int64\n",
      "bn4.weight torch.float32\n",
      "bn4.bias torch.float32\n",
      "bn4.running_mean torch.float32\n",
      "bn4.running_var torch.float32\n",
      "bn4.num_batches_tracked torch.int64\n",
      "bn5.weight torch.float32\n",
      "bn5.bias torch.float32\n",
      "bn5.running_mean torch.float32\n",
      "bn5.running_var torch.float32\n",
      "bn5.num_batches_tracked torch.int64\n",
      "bn6.weight torch.float32\n",
      "bn6.bias torch.float32\n",
      "bn6.running_mean torch.float32\n",
      "bn6.running_var torch.float32\n",
      "bn6.num_batches_tracked torch.int64\n",
      "fc.weight torch.float32\n",
      "fc.bias torch.float32\n",
      "---------end---------\n",
      "scale torch.float32\n",
      "conv1.weight torch.qint8\n",
      "conv1.bias torch.float32\n",
      "conv1.scale torch.float32\n",
      "conv2.weight torch.qint8\n",
      "conv2.bias torch.float32\n",
      "conv2.scale torch.float32\n",
      "conv3.weight torch.qint8\n",
      "conv3.bias torch.float32\n",
      "conv3.scale torch.float32\n",
      "conv4.weight torch.qint8\n",
      "conv4.bias torch.float32\n",
      "conv4.scale torch.float32\n",
      "conv5.weight torch.qint8\n",
      "conv5.bias torch.float32\n",
      "conv5.scale torch.float32\n",
      "conv6.weight torch.qint8\n",
      "conv6.bias torch.float32\n",
      "conv6.scale torch.float32\n",
      "fc.weight torch.qint8\n",
      "fc.bias torch.float32\n",
      "fc.scale torch.float32\n"
     ]
    }
   ],
   "source": [
    "#prints keys from quantized net\n",
    "qnet = QCifarNet()\n",
    "qsd = qnet.state_dict()\n",
    "for key in qsd: print(key, qsd[key].dtype)\n",
    "\n",
    "print('-------end------')\n",
    "\n",
    "sd = torch.load('state_dict.pt')\n",
    "\n",
    "for key in sd:print(key, sd[key].dtype)\n",
    "\n",
    "print('---------end---------')\n",
    "\n",
    "#-to-be-done- by student \n",
    "###\n",
    "###\n",
    "#-to-be-done- by student\n",
    "   \n",
    "for name, module in qnet.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        conv_name = name\n",
    "        bn_name = name.replace('conv', 'bn')\n",
    "        if bn_name in sd:\n",
    "            fused_conv_w, fused_conv_b = fuse_conv_bn_weights(\n",
    "                sd[conv_name + '.weight'], \n",
    "                sd[conv_name + '.bias'], \n",
    "                sd[bn_name + '.running_mean'], \n",
    "                sd[bn_name + '.running_var'], \n",
    "                sd[bn_name + '.weight'], \n",
    "                sd[bn_name + '.bias']\n",
    "            )\n",
    "            scale = tensor_scale(fused_conv_w)\n",
    "            qsd[conv_name + '.weight'] = torch.quantize_per_tensor(fused_conv, scale=scale, zero_point=0, dtype=torch.qint8)\n",
    "            \n",
    "            #qsd[conv_name + '.weight'] = fused_conv_w\n",
    "            #qsd[conv_name + '.bias'] = fused_conv_b        \n",
    "\n",
    "for key in qsd: print(key, qsd[key].dtype)\n",
    "torch.save(qsd, 'quantized_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2927e302-6f53-4bcf-8bc6-1edbb1006bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-dcc6201c0b1b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-dcc6201c0b1b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    layer_name2 =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#wrong code\n",
    "#for key in sd:\n",
    "#    if key.endswith('.weight'):\n",
    " #       layer_name = key[:-7]  # Remove '.weight' suffix to get layer name\n",
    "  #      conv_w = sd[key]\n",
    "   #     conv_b = sd[layer_name + '.bias']\n",
    "    #    layer_name2 = \n",
    "     #   # Fuse and quantize the convolution weights and biases with batch normalization parameters\n",
    "      #  fused_conv_w, fused_conv_b = fuse_conv_bn_weights(conv_w, conv_b, bn_rm[layer_name], bn_rv[layer_name], bn_w[layer_name], bn_b[layer_name])\n",
    "       # # Store the fused and quantized weights and biases into the quantized state_dict\n",
    "        #qsd[key] = fused_conv_w\n",
    "        #qsd[layer_name + '.bias'] = fused_conv_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-triangle",
   "metadata": {},
   "source": [
    "Now that we have the fused parameters, we still require the right scales for the activations. For that we \"observe\" the activation scales in the unquantized network using a calibration \"batch\", reuse the function `tensor_scale`\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>  \n",
    "* <span style=\"color:green\">Directly calculate the required scales in the forward pass, e.g. the scale for the inital quantization, and the output scale for each fused operation, and final output scale (the output of the FC layer).</span>\n",
    "* <span style=\"color:green\">There is already an inherited version of CifarNet provided, where you only have to redefine the forward pass and add the calculated scales to the `calibration_dict`. We will later use them to set the remaining scales in our quantized state_dict.</span>\n",
    "* <span style=\"color:green\">It is sufficient to estimate the scales in only one forward pass (we can make the batchsize large).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dutch-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarNetCalibration(CifarNet):\n",
    "    def __init__(self):\n",
    "        super(CifarNetCalibration, self).__init__()\n",
    "        \n",
    "        #we add a new dict for the corresponding scales\n",
    "        self.calibration_dict = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #to-be-done-by-student\n",
    "        ###\n",
    "        ###\n",
    "        ###\n",
    "        #--to---be---done---by---student\n",
    "        # Initial quantization scale\n",
    "        self.calibration_dict['input_scale'] = torch.tensor(tensor_scale(x))\n",
    "        \n",
    "        sd = self.state_dict()\n",
    "        convlayers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6]\n",
    "        \n",
    "        for layer in range(1,7):\n",
    "            c = 'conv' + str(layer)\n",
    "            b = 'bn' +str(layer)\n",
    "            \n",
    "            fused_conv, fused_bias = fuse_conv_bn_weights(sd[c + '.weight'], \n",
    "                                                          sd[c + '.bias'], \n",
    "                                                          sd[b + '.running_mean'], \n",
    "                                                          sd[b + '.running_var'], \n",
    "                                                          sd[b + '.weight'], \n",
    "                                                          sd[b + '.bias'])\n",
    "            \n",
    "                \n",
    "            convlayers[layer-1].weight = nn.Parameter(fused_conv)\n",
    "            convlayers[layer-1].bias = nn.Parameter(fused_bias)\n",
    "            \n",
    "            x = convlayers[layer-1](x)\n",
    "            \n",
    "            conv_scale = tensor_scale(x)\n",
    "            self.calibration_dict[c + '.weight'] = torch.quantize_per_tensor(fused_conv, scale = conv_scale, zero_point = 0, dtype = torch.qint8)\n",
    "            #self.calibration_dict[c + '.bias'] = fused_bias\n",
    "            self.calibration_dict[c + '.scale'] = conv_scale\n",
    "            \n",
    "            if layer % 2 == 0:\n",
    "                x = F.max_pool2d(x, 2, stride = 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        fc_scale = torch.tensor(tensor_scale(x))\n",
    "        self.calibration_dict['fc.weight'] = torch.quantize_per_tensor(sd['fc.weight'], scale = fc_scale, zero_point = 0, dtype = torch.qint8)\n",
    "        self.calibration_dict['fc.bias'] = sd['fc.bias']\n",
    "        self.calibration_dict['fc.scale'] = fc_scale\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fresh-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the calibration using a batch from the testdata\n",
    "net_calib = CifarNetCalibration()\n",
    "net_calib.load_state_dict(torch.load('state_dict.pt'))\n",
    "_, (data, _) = next(enumerate(testloader))\n",
    "net_calib(data)\n",
    "calibration_dict = net_calib.calibration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-sentence",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Your Task:</span>  \n",
    "* <span style=\"color:green\">Now, transfer the scales into the state_dict `qsd`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "usual-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: <class 'torch.Tensor'>\n",
      "conv1.weight: <class 'torch.Tensor'>\n",
      "conv1.bias: <class 'torch.Tensor'>\n",
      "conv1.scale: <class 'float'>\n",
      "conv2.weight: <class 'torch.Tensor'>\n",
      "conv2.bias: <class 'torch.Tensor'>\n",
      "conv2.scale: <class 'float'>\n",
      "conv3.weight: <class 'torch.Tensor'>\n",
      "conv3.bias: <class 'torch.Tensor'>\n",
      "conv3.scale: <class 'float'>\n",
      "conv4.weight: <class 'torch.Tensor'>\n",
      "conv4.bias: <class 'torch.Tensor'>\n",
      "conv4.scale: <class 'float'>\n",
      "conv5.weight: <class 'torch.Tensor'>\n",
      "conv5.bias: <class 'torch.Tensor'>\n",
      "conv5.scale: <class 'float'>\n",
      "conv6.weight: <class 'torch.Tensor'>\n",
      "conv6.bias: <class 'torch.Tensor'>\n",
      "conv6.scale: <class 'float'>\n",
      "fc.weight: <class 'torch.Tensor'>\n",
      "fc.bias: <class 'torch.Tensor'>\n",
      "fc.scale: <class 'torch.Tensor'>\n",
      "input_scale: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#-to-be-done- by student \n",
    "###\n",
    "###\n",
    "#-to-be-done- by student \n",
    "#qnet = CifarNet()  # Or your specific quantized network class\n",
    "#qsd = qnet.state_dict()\n",
    "\n",
    "# Transfer scales to the quantized state_dict\n",
    "#qsd['input_scale'] = calibration_dict['input_scale']\n",
    "#qsd['conv1_output_scale'] = calibration_dict['conv1_output_scale']\n",
    "#qsd['conv2_output_scale'] = calibration_dict['conv2_output_scale']\n",
    "#qsd['fc_output_scale'] = calibration_dict['fc_output_scale']\n",
    "#qsd['fc2_output_scale'] = calibration_dict['fc2_output_scale']\n",
    "for key in calibration_dict:\n",
    "    qsd[key] = calibration_dict[key]\n",
    "\n",
    "# Save the updated quantized state_dict\n",
    "#torch.save(qsd, 'quantized_state_dict_with_scales.pt')\n",
    "\n",
    "# Print the quantized state_dict with scales to verify\n",
    "for key, value in qsd.items():\n",
    "    print(f\"{key}: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "meaningful-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time quantized: 1.7280635833740234 s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-36e08c6ba2d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We run the accuracy test again to see how much accuracy we loose through quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Time quantized: {net_time(QCifarNet, testloader)} s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy quantized: {net_acc(QCifarNet, qsd, testloader):.4%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-26b1154aba69>\u001b[0m in \u001b[0;36mnet_acc\u001b[0;34m(model_class, state_dict, testloader)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#accuracy = test(model, test_loader, device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m#model.eval()  # Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1207\u001b[0m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0mlocal_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             module._load_from_state_dict(\n\u001b[0;32m-> 1204\u001b[0;31m                 state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n\u001b[0m\u001b[1;32m   1205\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mis_param_lazy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_param_lazy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                     \u001b[0minput_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#We run the accuracy test again to see how much accuracy we loose through quantization\n",
    "print(f'Time quantized: {net_time(QCifarNet, testloader)} s')\n",
    "print(f\"Accuracy quantized: {net_acc(QCifarNet, qsd, testloader):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-greece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
