{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22141721-912f-4e2e-8e24-ca60da9355a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tinyyolov2NoBN import TinyYoloV2NoBN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd53d017-ec81-4d88-b9bf-7bc90135fcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYoloV2NoBN(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv9): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TinyYoloV2NoBN(num_classes=1)\n",
    "sd = torch.load(\"fusedyolov2_0717.pt\")\n",
    "model.load_state_dict(sd, strict=False)\n",
    "#print(sd.keys())\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55ea470-cbb9-41ae-9825-11f964cd700b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYoloV2NoBN(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv9): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016b130f-5b50-43d1-ac1a-1e7fc7695c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.utils.prune as prune\n",
    "#parameters_to_prune = (\n",
    "#    (model.conv1, 'weight'),\n",
    "#    (model.conv2, 'weight'),\n",
    "#    (model.conv3, 'weight'),\n",
    "#    (model.conv4, 'weight'),\n",
    "#    (model.conv5, 'weight'),\n",
    "#    (model.conv6, 'weight'),\n",
    "    (model.conv7, 'weight'),\n",
    "    (model.conv8, 'weight'),\n",
    "    (model.conv9, 'weight'),\n",
    ")\n",
    "\n",
    "#for module, name in parameters_to_prune:\n",
    "    prune.ln_structured(module, name=name, amount=0.2, n=1, dim=0)  # 20% pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6319da-2706-4811-a1d3-fe8bf87b6142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyYoloV2NoBN(\n",
      "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv9): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#print(model.conv1.weight)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b237134f-d3e3-484d-871a-8118df2b21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prune.ln_structured(model.conv1, name='weight', amount=0.2, n=1, dim=0)\n",
    "\n",
    "# Print the first layer's weights after pruning\n",
    "#print(\"First layer weights after pruning:\")\n",
    "#print(model.conv1.weight)\n",
    "\n",
    "# Remove pruning reparametrizations to make the model dense\n",
    "#prune.remove(model.conv1, 'weight')\n",
    "\n",
    "# Print the first layer's weights after densifying\n",
    "#print(\"First layer weights after densifying:\")\n",
    "#print(model.conv1.weight)\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "#print(\"Original model:\")\n",
    "#print(model.conv1.weight)\n",
    "\n",
    "def pruning(model, amount=0.2, n=1, dim=0):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=n, dim=dim)  # 20% pruning      \n",
    "\n",
    "#pruning(model)\n",
    "#print(\"First layer weights after pruning:\")\n",
    "#print(model.conv1.weight)\n",
    "\n",
    "\n",
    "def remove_pruning(model):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, 'weight')\n",
    "            \n",
    "#remove_pruning(model)\n",
    "#print(\"First layer weights after densifying:\")\n",
    "#print(model.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5375ab47-9a9d-4a01-b53e-5399289593f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'densified_tinyyolov2NoBN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc9051c-4972-4f0a-93bb-0aa099a17b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss, _ = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d38cee6-38af-4540-b085-9d37272e4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from utils.loss import YoloLoss\n",
    "import tqdm\n",
    "\n",
    "def prune_and_retrain(model, train_loader, val_loader, num_iterations, pruning_amount, epochs_per_iteration):\n",
    "    criterion = YoloLoss(anchors=model.anchors)\n",
    "    optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=0.001)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Pruning iteration {iteration+1}/{num_iterations}\")\n",
    "        \n",
    "        pruning(model, amount=pruning_amount)\n",
    "        \n",
    "        #print(f\"Pruned.\")\n",
    "        model.train()\n",
    "        #print(f\"Trained.\")\n",
    "        for epoch in range(epochs_per_iteration):\n",
    "            running_loss = 0.0\n",
    "            print(f\"Begin Epoch {epoch}\")\n",
    "            for idx, (images, targets) in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                print(f\"Image, Target loaded.\")\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, yolo=False)\n",
    "                print(f\"Get Outputs.\")\n",
    "                loss, _ = criterion(outputs, targets)\n",
    "                print(f\"Get Loss.\")\n",
    "                loss.backward()\n",
    "                print(f\"Backwarded.\")\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{epochs_per_iteration}], Loss: {running_loss/len(train_loader)}\")\n",
    "            \n",
    "        evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        remove_pruning(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcbf45f-2111-4a7b-81ef-80f1160a58ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning iteration 1/5\n",
      "Begin Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2142 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image, Target loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2142 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bef750347583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCDataLoaderPerson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprune_and_retrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-dde8dd88c9d7>\u001b[0m in \u001b[0;36mprune_and_retrain\u001b[0;34m(model, train_loader, val_loader, num_iterations, pruning_amount, epochs_per_iteration)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image, Target loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Get Outputs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/embedded-ml-lab-students-ss24/exercises/4-challenge/tinyyolov2NoBN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, yolo)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# A subset of VOCDataLoader just for one class (person) (0)\n",
    "from utils.dataloader import VOCDataLoaderPerson\n",
    "\n",
    "loader = VOCDataLoaderPerson(train=True, batch_size=1, shuffle=True)\n",
    "loader_test = VOCDataLoaderPerson(train=False, batch_size=1)\n",
    "\n",
    "prune_and_retrain(model, loader, loader_test, 5, 0.2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82da6a5-8dd2-4e4d-a665-7a2181457393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# A subset of VOCDataLoader just for one class (person) (0)\n",
    "from utils.dataloader import VOCDataLoaderPerson\n",
    "\n",
    "loader = VOCDataLoaderPerson(train=True, batch_size=1, shuffle=True)\n",
    "loader_test = VOCDataLoaderPerson(train=False, batch_size=1)\n",
    "\n",
    "from tinyyolov2NoBN import TinyYoloV2NoBN\n",
    "from utils.loss import YoloLoss\n",
    "import tqdm\n",
    "\n",
    "# We define a tinyyolo network with only two possible classes\n",
    "net = TinyYoloV2NoBN(num_classes=1)\n",
    "sd = torch.load(\"fusedyolov2_0717.pt\", map_location=device)\n",
    "\n",
    "#We load all parameters from the pretrained dict except for the last layer\n",
    "#net.load_state_dict({k: v for k, v in sd.items() if not '9' in k}, strict=False)\n",
    "net.load_state_dict(sd, strict=False)\n",
    "net.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Definition of the loss\n",
    "criterion = YoloLoss(anchors=net.anchors)\n",
    "\n",
    "#We only train the last layer (conv9)\n",
    "#for key, param in net.named_parameters():\n",
    "#    if any(x in key for x in ['1', '2', '3', '4', '5', '6', '7']):\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, net.parameters()), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed021f4-1c50-4546-87c2-858185508499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning iteration 1/20\n",
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2142 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "from utils.ap import precision_recall_levels, ap, display_roc\n",
    "from utils.yolo import nms, filter_boxes\n",
    "\n",
    "NUM_TEST_SAMPLES = 30\n",
    "NUM_EPOCHS = 20\n",
    "test_AP = []\n",
    "num_iterations = 20\n",
    "ratio = 0.1\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Pruning iteration {iteration+1}/{num_iterations}\")\n",
    "        \n",
    "    pruning(net, amount=ratio)\n",
    "        \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch >= 0:\n",
    "            net.train()\n",
    "            net.to(device)\n",
    "            print(\"Training started.\")\n",
    "            for idx, (input, target) in tqdm.tqdm(enumerate(loader), total=len(loader)):\n",
    "                input, target = input.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                #Yolo head is implemented in the loss for training, therefore yolo=False\n",
    "                output = net(input, yolo=False)\n",
    "                loss, _ = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        net.eval()\n",
    "        print(\"Validation started.\")\n",
    "        with torch.no_grad():\n",
    "            for idx, (input, target) in tqdm.tqdm(enumerate(loader_test), total=NUM_TEST_SAMPLES):\n",
    "                input, target = input.to(device), target.to(device)\n",
    "                output = net(input, yolo=True)\n",
    "        \n",
    "                #The right threshold values can be adjusted for the target application\n",
    "                output = filter_boxes(output, 0.0)\n",
    "                output = nms(output, 0.5)\n",
    "        \n",
    "                precision, recall = precision_recall_levels(target[0], output[0])\n",
    "                test_precision.append(precision)\n",
    "                test_recall.append(recall)\n",
    "                if idx == NUM_TEST_SAMPLES:\n",
    "                    break\n",
    "                \n",
    "        #Calculation of average precision with collected samples\n",
    "        test_AP.append(ap(test_precision, test_recall))\n",
    "        print('average precision', test_AP)\n",
    "\n",
    "        #plot ROC\n",
    "        display_roc(test_precision, test_recall)\n",
    "        \n",
    "    remove_pruning(net)\n",
    "    state_dict = net.state_dict()\n",
    "        \n",
    "torch.save(state_dict, 'zhz_sr_pruned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cee912-5b01-43e4-a90c-526df2af5916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
