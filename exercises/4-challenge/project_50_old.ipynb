{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff99d896-d042-44f5-a180-b84e2debbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from utils.camera import CameraDisplay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a791f89-dd8b-467c-b5b8-940c2a6fe644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrunedTinyYoloV2NoBN(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(53, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(106, 210, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(210, 415, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(415, 825, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(825, 825, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(825, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "from pruned_tinyyolov2NoBN import PrunedTinyYoloV2NoBN\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "#model = TinyYoloV2(num_classes=20)\n",
    "model = PrunedTinyYoloV2NoBN(num_classes=1)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"models/voc_pruned_r002_10_times_19_epochs_lr0001_decay0005.pt\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43671032-632e-493b-b729-f1b1abf808d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "# Function to preprocess image for YOLOv2\n",
    "def preprocess(image):\n",
    "    # Resize to model input size, normalize, etc.\n",
    "    image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Change to CHW\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to postprocess YOLOv2 output\n",
    "def postprocess(output, conf_thresh=0.5, iou_thresh=0.4):\n",
    "    # Implement the postprocessing steps to get bounding boxes\n",
    "    # This is a simplified placeholder, adjust according to your model's output\n",
    "    boxes = []\n",
    "    # Assume output is in the shape of (batch_size, num_boxes, 5+num_classes)\n",
    "    output = output[0]  # Remove batch dimension\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_thresh:\n",
    "            box = detection[:4]  # Extract bounding box coordinates\n",
    "            x, y, w, h = box\n",
    "            x1 = int((x - w / 2) * 416)\n",
    "            y1 = int((y - h / 2) * 416)\n",
    "            x2 = int((x + w / 2) * 416)\n",
    "            y2 = int((y + h / 2) * 416)\n",
    "            boxes.append((x1, y1, x2, y2, confidence, class_id))\n",
    "    return boxes\n",
    "\n",
    "def display_result_img(image: np.ndarray, output: List[torch.Tensor]) -> np.ndarray:\n",
    "    ima_shape = image.shape[:2]\n",
    "    \n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0, i, -1] >= 0:\n",
    "                cx = int(bboxes[0, i, 0] * ima_shape[1])\n",
    "                cy = int(bboxes[0, i, 1] * ima_shape[0])\n",
    "                \n",
    "                w = int(bboxes[0, i, 2] * ima_shape[1])\n",
    "                h = int(bboxes[0, i, 3] * ima_shape[0])\n",
    "                \n",
    "                cv2.rectangle(image, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"Class {int(bboxes[0, i, 4])}\", (cx - w // 2, cy - h // 2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddbf48a-ebee-4d81-b41c-65b0c26e02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d2f411-dee3-4944-8389-c0f599488dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    # Change to CHW (channels, height, width)\n",
    "    #image = image.transpose(2, 0, 1)\n",
    "    #image = image[0:320, 0:320, :]\n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    input_image = preprocess(image)\n",
    "    \n",
    "    # Run the model\n",
    "    #with torch.no_grad():\n",
    "    #    output = model(input_image)\n",
    "    \n",
    "    # Postprocess the output\n",
    "    #boxes = postprocess(output)\n",
    "    \n",
    "    #image = torch.from_numpy(image).float().unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    output = filter_boxes(output, 0.1)\n",
    "    output = nms(output, 0.25)\n",
    "    #boxes = postprocess(output)\n",
    "    #boxes = output\n",
    "    #display_result(input, output, target\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    #for box in boxes:\n",
    "        # Draw the box and label on the image\n",
    "        #x1, y1, x2, y2, conf, cls = box\n",
    "        #cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #cv2.putText(image, f\"{cls}: {conf:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    image = display_result_img(image, output)\n",
    "    \n",
    "    # Draw FPS on the image\n",
    "    cv2.putText(image, \"fps=\" + fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3362b5-312d-4ed1-8982-85c33bb6c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db1e8f544774d96905bd856ee365a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08844f21-923c-4994-bb56-dfadb752d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e05412-ce32-45a3-9de7-7d50acee47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "cam.stop()\n",
    "cam.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
