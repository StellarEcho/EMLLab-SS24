{
 "cells": [
  {
   "cell_type": "raw",
   "id": "08d6ae9e-9c83-4d56-aadc-b4cc25edd0ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7d825-26fc-42f9-8187-33c8ddf6e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.dataloader import VOCDataLoaderPerson\n",
    "loader = VOCDataLoaderPerson(train=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf99ed4-cfe0-4d0c-be23-b7c3884ae6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "net = TinyYoloV2(num_classes=1)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"voc_pretrained.pt\")\n",
    "net.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e55ea-b2d1-4cce-a3d4-8ed816101481",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ef347-5bd8-42d4-8354-e6ad8ebbca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_and_bn(conv, bn):\n",
    "    with torch.no_grad():\n",
    "        # Fuse conv and bn layers\n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                    conv.out_channels,\n",
    "                                    kernel_size=conv.kernel_size,\n",
    "                                    stride=conv.stride,\n",
    "                                    padding=conv.padding,\n",
    "                                    bias=True).to(device)\n",
    "\n",
    "        # Prepare filters\n",
    "        w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var))).to(device)\n",
    "\n",
    "        fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.size()))\n",
    "\n",
    "        # Prepare spatial bias\n",
    "        if conv.bias is None:\n",
    "            b_conv = torch.zeros(conv.weight.size(0)).to(device)\n",
    "        else:\n",
    "            b_conv = conv.bias\n",
    "\n",
    "        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps)).to(device)\n",
    "\n",
    "        fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
    "\n",
    "        return fusedconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63628a0-b491-4ecc-9d15-94d2f7e3e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyyolov2NoBN import TinyYoloV2NoBN\n",
    "\n",
    "# Initialize the original model and load the weights\n",
    "original_modelnet = TinyYoloV2(num_classes=1).to(device)\n",
    "\n",
    "# load pretrained weights\n",
    "original_modelnet = torch.load(\"voc_pretrained.pt\")\n",
    "\n",
    "# Create a new model without BatchNorm layers\n",
    "fused_model = TinyYoloV2NoBN(num_classes=1).to(device)\n",
    "\n",
    "# Fuse each conv and bn layer\n",
    "fused_model.conv1 = fuse_conv_and_bn(original_model.conv1, original_model.bn1)\n",
    "fused_model.conv2 = fuse_conv_and_bn(original_model.conv2, original_model.bn2)\n",
    "fused_model.conv3 = fuse_conv_and_bn(original_model.conv3, original_model.bn3)\n",
    "fused_model.conv4 = fuse_conv_and_bn(original_model.conv4, original_model.bn4)\n",
    "fused_model.conv5 = fuse_conv_and_bn(original_model.conv5, original_model.bn5)\n",
    "fused_model.conv6 = fuse_conv_and_bn(original_model.conv6, original_model.bn6)\n",
    "fused_model.conv7 = fuse_conv_and_bn(original_model.conv7, original_model.bn7)\n",
    "fused_model.conv8 = fuse_conv_and_bn(original_model.conv8, original_model.bn8)\n",
    "\n",
    "# Copy the final conv layer directly (since it doesn't have BN)\n",
    "fused_model.conv9 = original_model.conv9\n",
    "\n",
    "#fused_model.eval()\n",
    "# Save the fused model state dict\n",
    "#fused_weights_path = 'path/to/your/fused_weights.pth'\n",
    "torch.save(fused_model.state_dict(), 'fusedyolov2.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d236497-e21c-4c84-950b-e4505d77b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fused_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f6f9e-80e0-48d2-bece-1d819a31d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for idx, (input, target) in tqdm.tqdm(enumerate(loader), total=len(loader)):\n",
    "    \n",
    "    #input is a 1 x 3 x 320 x 320 image\n",
    "    output = net(input)\n",
    "    \"output is of a tensor of size 32 x 125 x 10 x 10\"\n",
    "    #output is a 32 x 125 x 10 x 10 tensor\n",
    "    \n",
    "    #filter boxes based on confidence score (class_score*confidence)\n",
    "    output = filter_boxes(output, 0.1)\n",
    "    \n",
    "    #filter boxes based on overlap\n",
    "    output = nms(output, 0.25)\n",
    "    \n",
    "    display_result(input, output, target, file_path='yolo_prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81264fb-9e26-419b-8cdc-5f7e36723d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00831a-40c3-44ba-a7a9-10fb070c82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# A subset of VOCDataLoader just for one class (person) (0)\n",
    "from utils.dataloader import VOCDataLoaderPerson\n",
    "\n",
    "loader = VOCDataLoaderPerson(train=True, batch_size=1, shuffle=True)\n",
    "loader_test = VOCDataLoaderPerson(train=False, batch_size=1)\n",
    "\n",
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.loss import YoloLoss\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deb547-ad05-47b4-a45c-62a4e6631049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a tinyyolo network with only two possible classes\n",
    "net = TinyYoloV2(num_classes=1)\n",
    "sd = torch.load(\"voc_pretrained.pt\")\n",
    "\n",
    "#We load all parameters from the pretrained dict except for the last layer\n",
    "net.load_state_dict({k: v for k, v in sd.items() if not '9' in k}, strict=False)\n",
    "net.eval()\n",
    "\n",
    "# Definition of the loss\n",
    "criterion = YoloLoss(anchors=net.anchors)\n",
    "\n",
    "#We only train the last layer (conv9)\n",
    "for key, param in net.named_parameters():\n",
    "    if any(x in key for x in ['1', '2', '3', '4', '5', '6', '7']):\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, net.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b8a24-b8b9-442c-ab8b-fb7c1285ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ap import precision_recall_levels, ap, display_roc\n",
    "from utils.yolo import nms, filter_boxes\n",
    "\n",
    "NUM_TEST_SAMPLES = 5\n",
    "NUM_EPOCHS = 2\n",
    "test_AP = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    if epoch != 0:\n",
    "        for idx, (input, target) in tqdm.tqdm(enumerate(loader), total=len(loader)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Yolo head is implemented in the loss for training, therefore yolo=False\n",
    "            output = net(input, yolo=False)\n",
    "            loss, _ = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "    test_precision = []\n",
    "    test_recall = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (input, target) in tqdm.tqdm(enumerate(loader_test), total=NUM_TEST_SAMPLES):\n",
    "            output = net(input, yolo=True)\n",
    "            \n",
    "            #The right threshold values can be adjusted for the target application\n",
    "            output = filter_boxes(output, 0.0)\n",
    "            output = nms(output, 0.5)\n",
    "            \n",
    "            precision, recall = precision_recall_levels(target[0], output[0])\n",
    "            test_precision.append(precision)\n",
    "            test_recall.append(recall)\n",
    "            if idx == NUM_TEST_SAMPLES:\n",
    "                break\n",
    "                \n",
    "    #Calculation of average precision with collected samples\n",
    "    test_AP.append(ap(test_precision, test_recall))\n",
    "    print('average precision', test_AP)\n",
    "\n",
    "    #plot ROC\n",
    "    display_roc(test_precision, test_recall)\n",
    "    \n",
    "    state_dict = net.state_dict()\n",
    "    torch.save(state_dict, 'zhz_sr.pt')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0387b0-8342-412a-8dc7-1c0c67f3879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########open camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551519ca-42da-45a7-8fc9-8d9d3bb57404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.camera import CameraDisplay\n",
    "import time\n",
    "import cv2\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a3ada-3334-4b96-8286-af694a5be0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function (your detection pipeline)\n",
    "# Make sure to first load all your pipeline code and only at the end init the camera\n",
    "\n",
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    image = image[0:320,0:320, :]\n",
    "    cv2.putText(image, \"fps=\"+fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653471e-30d7-4318-8e4c-b3ba330a8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbe620-d8d9-423b-88cb-3fc397e7009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ad66a-aefa-42c3-88f2-cb93dcfa5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54fa23-9a45-458d-a968-b8d5e32ada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from utils.camera import CameraDisplay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171584a1-b674-4b17-a6d1-c24d49d051d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "model = TinyYoloV2(num_classes=20)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"voc_pretrained.pt\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb353a-916a-423e-b4bb-72a323049a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "# Function to preprocess image for YOLOv2\n",
    "def preprocess(image):\n",
    "    # Resize to model input size, normalize, etc.\n",
    "    image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Change to CHW\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to postprocess YOLOv2 output\n",
    "def postprocess(output, conf_thresh=0.5, iou_thresh=0.4):\n",
    "    # Implement the postprocessing steps to get bounding boxes\n",
    "    # This is a simplified placeholder, adjust according to your model's output\n",
    "    boxes = []\n",
    "    # Assume output is in the shape of (batch_size, num_boxes, 5+num_classes)\n",
    "    output = output[0]  # Remove batch dimension\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_thresh:\n",
    "            box = detection[:4]  # Extract bounding box coordinates\n",
    "            x, y, w, h = box\n",
    "            x1 = int((x - w / 2) * 416)\n",
    "            y1 = int((y - h / 2) * 416)\n",
    "            x2 = int((x + w / 2) * 416)\n",
    "            y2 = int((y + h / 2) * 416)\n",
    "            boxes.append((x1, y1, x2, y2, confidence, class_id))\n",
    "    return boxes\n",
    "\n",
    "def display_result_img(image: np.ndarray, output: List[torch.Tensor]) -> np.ndarray:\n",
    "    ima_shape = image.shape[:2]\n",
    "    \n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0, i, -1] >= 0:\n",
    "                cx = int(bboxes[0, i, 0] * ima_shape[1])\n",
    "                cy = int(bboxes[0, i, 1] * ima_shape[0])\n",
    "                \n",
    "                w = int(bboxes[0, i, 2] * ima_shape[1])\n",
    "                h = int(bboxes[0, i, 3] * ima_shape[0])\n",
    "                \n",
    "                cv2.rectangle(image, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"Class {int(bboxes[0, i, 4])}\", (cx - w // 2, cy - h // 2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba76116-1aea-4882-aed9-5164f92512d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869085ce-0bcf-4d8f-9f91-e2fa2d1fed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    # Change to CHW (channels, height, width)\n",
    "    #image = image.transpose(2, 0, 1)\n",
    "    #image = image[0:320, 0:320, :]\n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    input_image = preprocess(image)\n",
    "    \n",
    "    # Run the model\n",
    "    #with torch.no_grad():\n",
    "    #    output = model(input_image)\n",
    "    \n",
    "    # Postprocess the output\n",
    "    #boxes = postprocess(output)\n",
    "    \n",
    "    #image = torch.from_numpy(image).float().unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    output = filter_boxes(output, 0.1)\n",
    "    output = nms(output, 0.25)\n",
    "    #boxes = postprocess(output)\n",
    "    #boxes = output\n",
    "    #display_result(input, output, target\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    #for box in boxes:\n",
    "        # Draw the box and label on the image\n",
    "        #x1, y1, x2, y2, conf, cls = box\n",
    "        #cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #cv2.putText(image, f\"{cls}: {conf:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    image = display_result_img(image, output)\n",
    "    \n",
    "    # Draw FPS on the image\n",
    "    cv2.putText(image, \"fps=\" + fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa24248-c6a8-423f-9223-1bc719d285bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d647be-6a7a-4a18-b60d-a9068ad308af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ccfb1-c385-44c6-81db-01eaedba6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.stop()\n",
    "cam.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
