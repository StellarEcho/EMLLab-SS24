{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1536cc-91cd-45fb-8205-b2346eaec2f7",
   "metadata": {},
   "source": [
    "# Embedded ML Lab - Challenge (Camera example)\n",
    "\n",
    "This is an example notebook for the camera usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e44cdf-8743-4dc4-8ad0-77ec15525b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.camera import CameraDisplay\n",
    "import time\n",
    "import cv2\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f273cb05-1fa3-45cf-b637-66a6e22dbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function (your detection pipeline)\n",
    "# Make sure to first load all your pipeline code and only at the end init the camera\n",
    "\n",
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    image = image[0:320,0:320, :]\n",
    "    cv2.putText(image, \"fps=\"+fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3862254a-5f10-4d50-a109-fae94009e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fcfc8574a949068c133dcddb336925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef4f134f-6d8e-4e28-b9cd-ea47247856c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9baf3b9-a78c-4709-a1df-4fabd57366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7ce97fe-1dee-4ce2-a1ca-65afee501908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from utils.camera import CameraDisplay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "956af8e6-f8b7-4b22-bfef-c1a67a877779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYoloV2(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): Conv2d(1024, 125, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "model = TinyYoloV2(num_classes=20)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"voc_pretrained.pt\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2845d223-75f0-4ee1-9c77-eeeb3dedebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "# Function to preprocess image for YOLOv2\n",
    "def preprocess(image):\n",
    "    # Resize to model input size, normalize, etc.\n",
    "    image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Change to CHW\n",
    "    image = torch.from_numpy(image).float().unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to postprocess YOLOv2 output\n",
    "def postprocess(output, conf_thresh=0.5, iou_thresh=0.4):\n",
    "    # Implement the postprocessing steps to get bounding boxes\n",
    "    # This is a simplified placeholder, adjust according to your model's output\n",
    "    boxes = []\n",
    "    # Assume output is in the shape of (batch_size, num_boxes, 5+num_classes)\n",
    "    output = output[0]  # Remove batch dimension\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_thresh:\n",
    "            box = detection[:4]  # Extract bounding box coordinates\n",
    "            x, y, w, h = box\n",
    "            x1 = int((x - w / 2) * 416)\n",
    "            y1 = int((y - h / 2) * 416)\n",
    "            x2 = int((x + w / 2) * 416)\n",
    "            y2 = int((y + h / 2) * 416)\n",
    "            boxes.append((x1, y1, x2, y2, confidence, class_id))\n",
    "    return boxes\n",
    "\n",
    "def display_result_img(image: np.ndarray, output: List[torch.Tensor]) -> np.ndarray:\n",
    "    ima_shape = image.shape[:2]\n",
    "    \n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0, i, -1] >= 0:\n",
    "                cx = int(bboxes[0, i, 0] * ima_shape[1])\n",
    "                cy = int(bboxes[0, i, 1] * ima_shape[0])\n",
    "                \n",
    "                w = int(bboxes[0, i, 2] * ima_shape[1])\n",
    "                h = int(bboxes[0, i, 3] * ima_shape[0])\n",
    "                \n",
    "                cv2.rectangle(image, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"Class {int(bboxes[0, i, 4])}\", (cx - w // 2, cy - h // 2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9b4054e-f8a5-48bb-ada2-c99d54f292f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07ccaeaa-3e06-4726-99f7-f1c957fbf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    # Change to CHW (channels, height, width)\n",
    "    #image = image.transpose(2, 0, 1)\n",
    "    #image = image[0:320, 0:320, :]\n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    input_image = preprocess(image)\n",
    "    \n",
    "    # Run the model\n",
    "    #with torch.no_grad():\n",
    "    #    output = model(input_image)\n",
    "    \n",
    "    # Postprocess the output\n",
    "    #boxes = postprocess(output)\n",
    "    \n",
    "    #image = torch.from_numpy(image).float().unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    output = filter_boxes(output, 0.1)\n",
    "    output = nms(output, 0.25)\n",
    "    #boxes = postprocess(output)\n",
    "    #boxes = output\n",
    "    #display_result(input, output, target\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    #for box in boxes:\n",
    "        # Draw the box and label on the image\n",
    "        #x1, y1, x2, y2, conf, cls = box\n",
    "        #cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #cv2.putText(image, f\"{cls}: {conf:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    image = display_result_img(image, output)\n",
    "    \n",
    "    # Draw FPS on the image\n",
    "    cv2.putText(image, \"fps=\" + fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91f4d724-e150-43cf-b657-190769a47a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71f731d9a474a07aec0568ac4827083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b5c6d23-86da-42f2-9a7b-15fae02fb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d355b10-3ae4-4ebb-927e-21b9b6b4ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "cam.stop()\n",
    "cam.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
