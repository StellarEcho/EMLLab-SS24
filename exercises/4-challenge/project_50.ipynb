{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff99d896-d042-44f5-a180-b84e2debbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from utils.camera import CameraDisplay\n",
    "import numpy as np\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"onnx/voc_pruned_r002_10_times_19_epochs_lr0001_decay0005.onnx\")\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "#output_name = ort_session.get_outputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "\n",
    "from typing import List \n",
    "# Function to preprocess image for YOLOv2\n",
    "def preprocess(image, input_shape):\n",
    "    # Resize to model input size, normalize, etc.\n",
    "    image = cv2.resize(image, (input_shape[2], input_shape[3]))\n",
    "    #image = cv2.resize(image, (416, 416))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Change to CHW\n",
    "    #image = torch.from_numpy(image).float().unsqueeze(0)  # Add batch dimension\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    #return image\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "# Function to postprocess YOLOv2 output\n",
    "def postprocess(output, conf_thresh=0.5, iou_thresh=0.4):\n",
    "    # Implement the postprocessing steps to get bounding boxes\n",
    "    # This is a simplified placeholder, adjust according to your model's output\n",
    "    boxes = []\n",
    "    # Assume output is in the shape of (batch_size, num_boxes, 5+num_classes)\n",
    "    output = output[0]  # Remove batch dimension\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_thresh:\n",
    "            box = detection[:4]  # Extract bounding box coordinates\n",
    "            x, y, w, h = box\n",
    "            x1 = int((x - w / 2) * 416)\n",
    "            y1 = int((y - h / 2) * 416)\n",
    "            x2 = int((x + w / 2) * 416)\n",
    "            y2 = int((y + h / 2) * 416)\n",
    "            boxes.append((x1, y1, x2, y2, confidence, class_id))\n",
    "    return boxes\n",
    "\n",
    "def display_result_img(image: np.ndarray, output: List[torch.Tensor]) -> np.ndarray:\n",
    "    ima_shape = image.shape[:2]\n",
    "    \n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0, i, -1] >= 0:\n",
    "                cx = int(bboxes[0, i, 0] * ima_shape[1])\n",
    "                cy = int(bboxes[0, i, 1] * ima_shape[0])\n",
    "                \n",
    "                w = int(bboxes[0, i, 2] * ima_shape[1])\n",
    "                h = int(bboxes[0, i, 3] * ima_shape[0])\n",
    "                \n",
    "                cv2.rectangle(image, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"Class {int(bboxes[0, i, 4])}\", (cx - w // 2, cy - h // 2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d2f411-dee3-4944-8389-c0f599488dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "\n",
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    # Change to CHW (channels, height, width)\n",
    "    #image = image.transpose(2, 0, 1)\n",
    "    #image = image[0:320, 0:320, :]\n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    input_image = preprocess(image, input_shape)\n",
    "    \n",
    "    #ort_inputs = {input_name: input_image}\n",
    "    ort_outs = session.run(None, {input_name: input_image})\n",
    "    \n",
    "    # Run the model\n",
    "    #with torch.no_grad():\n",
    "    #    output = model(input_image)\n",
    "    \n",
    "    # Postprocess the output\n",
    "    #boxes = postprocess(output)\n",
    "    \n",
    "    #image = torch.from_numpy(image).float().unsqueeze(0)\n",
    "    \n",
    "    output = ort_outs[0]\n",
    "    output = torch.Tensor(output)\n",
    "    #with torch.no_grad():\n",
    "    #    output = model(input_image)\n",
    "    output = filter_boxes(output, 0.10)\n",
    "    output = nms(output, 0.4)\n",
    "    #output = postprocess(output, 0.5, 0.3)\n",
    "    #boxes = postprocess(output)\n",
    "    #boxes = output\n",
    "    #display_result(input, output, target\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    #for box in boxes:\n",
    "        # Draw the box and label on the image\n",
    "        #x1, y1, x2, y2, conf, cls = box\n",
    "        #cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #cv2.putText(image, f\"{cls}: {conf:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    image = display_result_img(image, output)\n",
    "    \n",
    "    # Draw FPS on the image\n",
    "    cv2.putText(image, \"fps=\" + fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3362b5-312d-4ed1-8982-85c33bb6c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d090918300604807bedcc6a9279cf1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08844f21-923c-4994-bb56-dfadb752d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e05412-ce32-45a3-9de7-7d50acee47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "cam.stop()\n",
    "cam.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
